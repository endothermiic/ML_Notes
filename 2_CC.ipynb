{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "The following notebook is a concise summary of \"Machine Learning Crash Course\" - the 2nd course in [Google's Machine Learning Series](https://developers.google.com/machine-learning). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Recap\n",
    "\n",
    "Recall key terminology:\n",
    "\n",
    "* Label ($y$) - variable we're predicting \n",
    "* Feature ($x_1, x_2 ... x_n$) - input variables that describe out data \n",
    "\n",
    "Models map instances of data ($x_n$) to predicted labels ($y'$). \n",
    "\n",
    "Supervised learning typically involes either regression (continuous values predicted) or classification (discrete values predicted) models. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression & Loss Introduction\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:597/1*RqL8NLlCpcTIzBcsB-3e7A.png\" height=\"70%\" width=\"50%\">\n",
    "\n",
    "If $\\vec{x} = (x_1, ... x_n) \\in \\mathbb{R}^D$, where $\\vec{x}$ is a vector of features, our goal in using a regression model is to make predictions $y$ that are as close to the target $t$ as possible. For a linear regression, $y = \\sum_{i} w_ix_i + b$, where: \n",
    "* $y$ is the prediction \n",
    "* $\\vec{w}$ is the weight vector \n",
    "* $b$ is the bias \n",
    "\n",
    "A common loss function is \"squared (L2) loss,\" with $$L_2 = \\sum(y - y')^2,$$ where $y$ is the observed value and $y'$ is the predicted value. \n",
    "\n",
    "The \"mean square error\" is simply the average L2 loss over the entire dataset, or $$ \\frac{1}{n} \\sum_{(x, y) \\in D}(y - y')^2, $$ where $n$ is the number of data points, $x$ is a feature or set of features, $y$ is the label and $D$ is the dataset (with $(x, y)$ pairs).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "<img src = \"https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentDiagram.svg\" style=\"background-color: white\">\n",
    "\n",
    "We take an iterative approach to reducing loss. Models use features to generate a predicted label ($y' = w_1x_1 + b$). This is compared to the true label from the dataset to determine loss. Since our goal is to minimize loss, the model 'updates' the parameters $b$ and $w$ over and over again until loss is minimized. \n",
    "\n",
    "However, the process of 'updating' is still a black box at this stage. How does our model know *how* to update the parameters? How does it ensure loss is minimized? The most common mechanism is known as gradient descent.\n",
    "\n",
    "Given the function $f(x_1,...,x_n) \\in \\mathbb{R}^n$, $f$ has a partial derivative $\\frac{\\partial f}{\\partial x_i}$. At a given point $a$, these derivatives define the vector\n",
    "\n",
    "$$\\nabla f(a) = \\left(\\frac{\\partial f}{\\partial {x_1}}(a),...,\\frac{\\partial f}{\\partial x_n}(a)\\right),$$\n",
    "\n",
    "which is also called the *gradient* of $f$ at point $a$. Weights are initialized to 'reasonable' (often trivial) values, and adjusted in the \"direction of steepest descent\" of the gradient.  \n",
    "\n",
    "Subsequent gradients are calculated by multiplying the current gradient by the learning rate ($\\alpha$).\n",
    " * For one-dimensional functions, the ideal learning rate is $\\frac{1}{f''(x)}$ \n",
    " * For higher dimensional functions, the ideal learning rate is inverse of the Hessian\n",
    "\n",
    "Note: The Hessian matrix $H_f$ is defined as $ \\nabla^2 f $ or:\n",
    "\n",
    "\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data points used to calculate a given gradient is known as a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (v3.10.1:2cd268a3a9, Dec  6 2021, 14:28:59) [Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
