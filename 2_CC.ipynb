{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "The following notebook is a concise summary of \"Machine Learning Crash Course\" - the 2nd course in [Google's Machine Learning Series](https://developers.google.com/machine-learning). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning: Recap\n",
    "\n",
    "Recall key terminology:\n",
    "\n",
    "* Label ($y$) - variable we're predicting \n",
    "* Feature ($x_1, x_2 ... x_n$) - input variables that describe out data \n",
    "\n",
    "Models map instances of data ($x_n$) to predicted labels ($y'$). \n",
    "\n",
    "Supervised learning typically involes either regression (continuous values predicted) or classification (discrete values predicted) models. When dealing with supervised learning models, we assume that: \n",
    "* Examples are drawn independently and identically (i.i.d), at random, from the same distribution\n",
    "* Distribution is stationary (no changes within the dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression & Loss Introduction\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:597/1*RqL8NLlCpcTIzBcsB-3e7A.png\" height=\"70%\" width=\"50%\">\n",
    "\n",
    "If $\\vec{x} = (x_1, ... x_n) \\in \\mathbb{R}^D$, where $\\vec{x}$ is a vector of features, our goal in using a regression model is to make predictions $y$ that are as close to the target $t$ as possible. For a linear regression, $y = \\sum_{i} w_ix_i + b$, where: \n",
    "* $y$ is the prediction \n",
    "* $\\vec{w}$ is the weight vector \n",
    "* $b$ is the bias \n",
    "\n",
    "A common loss function is \"squared (L2) loss,\" with $$L_2 = \\sum(y - y')^2,$$ where $y$ is the observed value and $y'$ is the predicted value. \n",
    "\n",
    "The \"mean square error\" is simply the average L2 loss over the entire dataset, or $$ \\frac{1}{n} \\sum_{(x, y) \\in D}(y - y')^2, $$ where $n$ is the number of data points, $x$ is a feature or set of features, $y$ is the label and $D$ is the dataset (with $(x, y)$ pairs).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "<img src = \"https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentDiagram.svg\" style=\"background-color: white\">\n",
    "\n",
    "We take an iterative approach to reducing loss. Models use features to generate a predicted label ($y' = w_1x_1 + b$). This is compared to the true label from the dataset to determine loss. Since our goal is to minimize loss, the model 'updates' the parameters $b$ and $w$ over and over again until loss is minimized. \n",
    "\n",
    "However, the process of 'updating' is still a black box at this stage. How does our model know *how* to update the parameters? How does it ensure loss is minimized? The most common mechanism is known as gradient descent.\n",
    "\n",
    "Given the function $f(x_1,...,x_n) \\in \\mathbb{R}^n$, $f$ has a partial derivative $\\frac{\\partial f}{\\partial x_i}$. At a given point $a$, these derivatives define the vector\n",
    "\n",
    "$$\\nabla f(a) = \\left(\\frac{\\partial f}{\\partial {x_1}}(a),...,\\frac{\\partial f}{\\partial x_n}(a)\\right),$$\n",
    "\n",
    "which is also called the *gradient* of $f$ at point $a$. Weights are initialized to 'reasonable' (often trivial) values, and adjusted in the \"direction of steepest descent\" of the gradient.  \n",
    "\n",
    "Subsequent gradients are calculated by multiplying the current gradient by the learning rate ($\\alpha$).\n",
    " * For one-dimensional functions, the ideal learning rate is $\\frac{1}{f''(x)}$ \n",
    " * For higher dimensional functions, the ideal learning rate is inverse of the Hessian\n",
    "\n",
    "Note: The Hessian matrix $H_f$ is defined as $ \\nabla^2 f $ or:\n",
    "\n",
    "\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of data points used to calculate a given gradient is known as a batch. By reducing the batch size, we can increase efficiency. In processes such as stochastic gradient descent (SGD), each iteration uses a batch size of 1, randomly chosen (i.e. 1 example). Others, such as mini-batch SGD, uses batches from 10-1000, also randomly chosen. \n",
    "\n",
    "Note: This is distinct from an epoch, which refers to the number of iterations the model makes through a training dataset. \n",
    "\n",
    "Below are some important suggestions for tuning hyperparameters (epochs, learning rates and batch size): \n",
    "* Loss should initially decrease rapidly, then reach a horizontal asymptote\n",
    "    * If this convergence doens't happen, increase epochs\n",
    "    * If loss decreases slowly, increase $ \\alpha $\n",
    "    * If loss varies, decrease the learning $ \\alpha $\n",
    "* Begin with large batch sizes, and decrease as far as possible without degredation of results\n",
    "* Low $ \\alpha $ with higher epochs or batch size often works well \n",
    "\n",
    "When training a model, it's important to avoid *overfitting*. The model below has minimal loss, but still serves as a bad model: new data would likely be miscategorized. \n",
    "\n",
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/1200px-Overfitting.svg.png\" width=\"70%\">\n",
    "\n",
    "In other words, the best models are usually simple (see: Ockham's Razor). To ensure you aren't overfitting, it's important to split data into distinct sets: \n",
    "* Training set: data used to train your model (~70%)\n",
    "* Validation set: data used to tune your model (~15%)\n",
    "* Test set: data used to confirm results (~15%)\n",
    "\n",
    "Validation and test sets should be representative of the entire data set and large enough to yield statistically significant results. Minimal exposure of the model to the test set and prevent overfitting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features In-Detail\n",
    "\n",
    "\"Feature Enginering\" refers to the process of converting raw data to a feature vector. \n",
    "\n",
    "Good values for feature vectors are:\n",
    "* \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy & Pandas Review\n",
    "\n",
    "Note: ```array``` (NumPy), ```list``` (Python) and ```tensor``` (Tensorflow) are all analagous structures to matrices.\n",
    "\n",
    "The following code snippets will deal with array creation and manipulation in NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "<function ones at 0x117e30670>\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[0.16162712 0.68256191 0.57716913 0.35841684]\n",
      "[52 82 78 42 29 46 62 84]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Create a 2D array with selected values\n",
    "new_array = np.array([[1, 2], [3, 4]])\n",
    "print(new_array)\n",
    "\n",
    "# Create an array of all 1s \n",
    "zero_array = np.ones\n",
    "print(zero_array)\n",
    "\n",
    "# Create an array of nums from x (inclusive) to y (not inclusive)\n",
    "set_array = np.arange(6, 21)\n",
    "print(set_array)\n",
    "\n",
    "# Create an array of random floating pts from 0 to 1\n",
    "floats = np.random.random([4])\n",
    "print(floats)\n",
    "\n",
    "# Create an array of random nums from x (inclusive) to y (not inclusive)\n",
    "rand_array = np.random.randint(low=20, high=100, size=(8))\n",
    "print(rand_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippets will focus on pandas (note: in pandas, DataFrames are the main data structure - this is explored below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   temp  light\n",
      "0    30     20\n",
      "1    20     10\n",
      "2    10      5\n",
      "3     0      0 \n",
      "\n",
      "   temp  light  new col\n",
      "0    30     20       40\n",
      "1    20     10       20\n",
      "2    10      5       10\n",
      "3     0      0        0 \n",
      "\n",
      "   temp  light  new col\n",
      "0    30     20       40\n",
      "1    20     10       20 \n",
      "\n",
      "   temp  light  new col\n",
      "0    30     20       40 \n",
      "\n",
      "   temp  light  new col\n",
      "0    30     20       40\n",
      "1    20     10       20\n",
      "2    10      5       10 \n",
      "\n",
      "   temp  light  new col\n",
      "2    10      5       10\n",
      "3     0      0        0\n",
      "1    20     10       20\n",
      "0    30     20       40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create array to store temp vs light data\n",
    "my_data = np.array([[30, 20], [20, 10], [10, 5], [0, 0]])\n",
    "\n",
    "# Create cols\n",
    "my_cols = ['temp', 'light']\n",
    "\n",
    "# Create and print \n",
    "dataframe = pd.DataFrame(data=my_data, columns=my_cols)\n",
    "print(dataframe, '\\n')\n",
    "\n",
    "# Create a new column based on doubled light values\n",
    "dataframe[\"new col\"] = dataframe[\"light\"] * 2\n",
    "print(dataframe, '\\n')\n",
    "\n",
    "# Print the first n rows (in this case, 2)\n",
    "print(dataframe.head(2), '\\n')\n",
    "\n",
    "# Print the nth row (index starts at 0) (in this case, print the first)\n",
    "print(dataframe.iloc[[0]], '\\n')\n",
    "\n",
    "# Print rows 0 (inclusive) to 3 (exclusive)\n",
    "print(dataframe[0:3], '\\n')\n",
    "\n",
    "# Copy a DataFrame - creates an INDEPENDENT copy\n",
    "copy = pd.DataFrame.copy\n",
    "\n",
    "# Reference a DataFrame (simply assign to another var) \n",
    "ref = dataframe\n",
    "\n",
    "# Shuffle rows (could be helpful if to ensure test/validation/training datasets differ)\n",
    "shuffled = dataframe.reindex(np.random.permutation(dataframe.index))\n",
    "print(shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
